{
 "metadata": {
  "name": "",
  "signature": "sha256:575fc46e944a2236dc5f126aeeb496aee0d4a008f2b54989cd9732ace5ff79fb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import re\n",
      "from collections import Counter\n",
      "import os\n",
      "import json\n",
      "from IPython.display import clear_output\n",
      "import sys\n",
      "from math import sqrt, log\n",
      "\n",
      "from stemming.porter2 import stem\n",
      "\n",
      "from pprint import pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_dir = \"E:/LocalData/PubMed/Cell/\"\n",
      "\n",
      "stop_words = [w.replace(\"\\n\", \"\") for w in open(\"stop_words.txt\")]\n",
      "\n",
      "unstemmed = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def file_root(f):\n",
      "    tree = ET.parse(f)\n",
      "    return tree.getroot()\n",
      "\n",
      "def get_abstract_node(root):\n",
      "    return root.iter(\"abstract\").next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(node):\n",
      "\n",
      "    ts = [get_text(n) for n in node]\n",
      "    \n",
      "    txts = [node_text for sublist in ts for node_text in sublist]\n",
      "\n",
      "    if node.text:\n",
      "        txts.append(node.text)\n",
      "    return txts\n",
      "\n",
      "def is_float(f):\n",
      "    try:\n",
      "        float(f)\n",
      "        return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "def words(text_list):\n",
      "    word_list = [re.split('\\W+',t) for t in text_list]\n",
      "    \n",
      "    words = [unicode(word).lower() for text in word_list for word in text]\n",
      "    \n",
      "    # Filter out words that are too short.\n",
      "    words = [w for w in words if len(w) > 2]\n",
      "    \n",
      "    # Filter out words that are just numbers.\n",
      "    words = [w for w in words if not is_float(w)]\n",
      "    \n",
      "    # Filter out words that start with a number.\n",
      "    words = [w for w in words if not is_float(w[0])]\n",
      "    \n",
      "    # Filter out stop words\n",
      "    words = [w for w in words if not w in stop_words]\n",
      "    \n",
      "    # Do stemming\n",
      "    stemmed = [(w, stem(w)) for w in words]\n",
      "    \n",
      "    # Save map of stem -> original\n",
      "    for w,s in stemmed:\n",
      "        unstemmed[s] = w\n",
      "    \n",
      "    return [s for w,s in stemmed]\n",
      "\n",
      "def normalise_counter(c):\n",
      "    total_words = sum(c.values())\n",
      "    \n",
      "    for a in c:\n",
      "        c[a] /= float(total_words)\n",
      "\n",
      "def process_file(f):\n",
      "    \n",
      "    root = file_root(f)\n",
      "    \n",
      "    try:\n",
      "        abstract = get_abstract_node(root)\n",
      "    except:\n",
      "        return Counter()\n",
      "    \n",
      "    all_words = words(get_text(abstract))\n",
      "    \n",
      "    return Counter(all_words)\n",
      "\n",
      "def process_all_files():\n",
      "    \n",
      "    corpus_words = Counter()\n",
      "    \n",
      "    files = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
      "\n",
      "    word_files = {}\n",
      "    file_words = {}\n",
      "    for f in files:\n",
      "        clear_output()\n",
      "        print \"Processing %s\" % f\n",
      "        sys.stdout.flush()\n",
      "        \n",
      "        file_words[f] = process_file(f)\n",
      "        corpus_words.update(file_words[f])\n",
      "        for k in file_words[f]:\n",
      "            word_files[k] = word_files.get(k, set())\n",
      "            word_files[k].add(f)\n",
      "            \n",
      "    return file_words, corpus_words, word_files, files\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_words, corpus_words, word_files, files = process_all_files()\n",
      "\n",
      "print \"Computed word count for entire corpus\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Per-paper feature vector\n",
      "\n",
      "For the feature vector, only keep words that occur in at least 3 papers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Only allow words that occur in at least three papers\n",
      "\n",
      "allowed_words = [w for w in word_files if len(word_files[w]) >= 3]\n",
      "\n",
      "feature_vectors = []\n",
      "for f in files:\n",
      "    counter = Counter()\n",
      "    for w in file_words[f]:\n",
      "        if w in allowed_words:\n",
      "            counter[w] = file_words[f][w]\n",
      "            \n",
      "    feature_vectors.append(counter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# K-Means"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance(fv1, fv2):\n",
      "    all_features = set([f for f in fv1]).union([f for f in fv2])\n",
      "    \n",
      "    s = 0\n",
      "    for f in all_features:\n",
      "        s += (fv1.get(f,0) - fv2.get(f,0)) ** 2\n",
      "        \n",
      "    return sqrt(s)\n",
      "\n",
      "def centroid(fvs):\n",
      "    \n",
      "    centre = Counter()\n",
      "    \n",
      "    for f in fvs:\n",
      "        centre += f\n",
      "        \n",
      "    for a in centre:\n",
      "        centre[a] /= float(len(fvs))\n",
      "        \n",
      "    return centre\n",
      "\n",
      "def nearest_centroid_index(fv, centroids):\n",
      "    \n",
      "    min_dist = distance(fv, centroids[0])\n",
      "    nearest_index = 0\n",
      "    \n",
      "    for i in range(1, len(centroids)):\n",
      "        d = distance(fv, centroids[i])\n",
      "        if d < min_dist:\n",
      "            min_dist = d\n",
      "            nearest_index = i\n",
      "    \n",
      "    return nearest_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def kmeans(xs,K):\n",
      "    \"\"\"xs is a vector of Counters. K is the required number of clusters\"\"\"\n",
      "    \n",
      "    N = len(xs)\n",
      "    seeds = xs[:K] # Pick first K papers as seed cluster centroids\n",
      "\n",
      "    # Initialise centroids\n",
      "    mu = seeds\n",
      "\n",
      "    # Initialise cluster members\n",
      "    omega = []\n",
      "    for i in range(K):\n",
      "        omega.append(set())\n",
      "\n",
      "\n",
      "    last_omega = omega[:]\n",
      "    i = 0\n",
      "    while i < 20:\n",
      "        for k in range(K):\n",
      "            omega[k] = set()\n",
      "\n",
      "        # Reassign xs to nearest centroid\n",
      "\n",
      "        for n in range(N):\n",
      "            j = nearest_centroid_index(xs[n], mu)\n",
      "            omega[j].add(n)\n",
      "\n",
      "        # Recompute cluster centres\n",
      "\n",
      "        for k in range(K):\n",
      "            mu[k] = centroid([xs[t] for t in omega[k]])\n",
      "\n",
      "\n",
      "        match = True\n",
      "        for t in range(len(omega)):\n",
      "            if omega[t] != last_omega[t]:\n",
      "                match = False\n",
      "                break\n",
      "\n",
      "        if match:\n",
      "            print \"\\nConverged in %d iterations.\\n\\n\" % i\n",
      "            break \n",
      "\n",
      "        print\n",
      "        print str(i) + \": \" + str(omega)\n",
      "\n",
      "        i += 1\n",
      "\n",
      "        last_omega = omega[:]\n",
      "    \n",
      "    return omega\n",
      "    \n",
      "\n",
      "def mutual_information(word, cluster, files, feature_vectors):\n",
      "    N = float(len(files))\n",
      "    \n",
      "    # The number of files not containing the word and not in the cluster\n",
      "    N00 = float(len([f for fi,f in enumerate(files) if not fi in cluster and not word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files containing the word and in the cluster\n",
      "    N11 = float(len([f for fi,f in enumerate(files) if fi in cluster and word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files containing the word, but not in the cluster\n",
      "    N10 = float(len([f for fi,f in enumerate(files) if not fi in cluster and word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files not containing the word, but in the cluster\n",
      "    N01 = float(len([f for fi,f in enumerate(files) if fi in cluster and not word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files containing the word\n",
      "    N1_ = N11 + N10\n",
      "    \n",
      "    # The number of files not containing the word\n",
      "    N0_ = N00 + N01\n",
      "    \n",
      "    # The number of files in the cluster\n",
      "    N_1 = N01 + N11\n",
      "    \n",
      "    # The number of files not in the cluster\n",
      "    N_0 = N00 + N10\n",
      "    \n",
      "    #print N00, N01, N10, N11\n",
      "    \n",
      "    mi = 0\n",
      "    if N11 > 0:\n",
      "        mi += N11/N * log(N*N11/(N1_*N_1),2)\n",
      "    if N01 > 0:\n",
      "        mi += N01/N * log(N*N01/(N0_*N_1),2)\n",
      "    if N10 > 0:\n",
      "        mi += N10/N * log(N*N10/(N1_*N_0),2)\n",
      "    if N00 > 0:\n",
      "        mi += N00/N * log(N*N00/(N0_*N_0),2)\n",
      "    \n",
      "    return mi\n",
      "         \n",
      "    \n",
      "def cluster_label(cluster, feature_vectors, files):\n",
      "    \n",
      "    cluster_words = Counter()\n",
      "    for fi in cluster:\n",
      "        cluster_words += feature_vectors[fi]\n",
      "        \n",
      "    for w in cluster_words:\n",
      "        cluster_words[w] = mutual_information(w, cluster, files, feature_vectors)\n",
      "    \n",
      "    return cluster_words.most_common(1)[0][0]\n",
      "            \n",
      "          \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "clusters = kmeans(feature_vectors, 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels = []\n",
      "for cluster in clusters:\n",
      "    if len(cluster) < 2:\n",
      "        continue # Ignore singleton clusters\n",
      "        \n",
      "    label = cluster_label(cluster, feature_vectors, files)\n",
      "    labels.append(label)\n",
      "    \n",
      "pprint(labels)\n",
      "\n",
      "#mutual_information(\"protein\", clusters[7], files, file_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This used to generate labels based on most common word in cluster.\n",
      "\n",
      "#labels =  []\n",
      "#\n",
      "#for k in range(len(clusters)):\n",
      "#    total_word_freq = Counter()\n",
      "#    \n",
      "#    if len(clusters[k]) < 2:\n",
      "#        continue # Ignore clusters with only one paper\n",
      "#        \n",
      "#    for fi in clusters[k]:\n",
      "#        total_word_freq += file_words[files[fi]]\n",
      "#    \n",
      "#    labels.append(total_word_freq.most_common(1)[0][0])\n",
      "#\n",
      "#pprint(labels)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def files_with_both_keywords(kw1, kw2):\n",
      "    return word_files[kw1].intersection(word_files[kw2])\n",
      "\n",
      "\n",
      "def calculate_kw_matrix():\n",
      "\n",
      "    kw_matrix = {}\n",
      "    kw_papers = {}\n",
      "\n",
      "    for kw1 in all_kws:\n",
      "        clear_output()\n",
      "        print \"Testing %s\" % kw1\n",
      "        sys.stdout.flush()\n",
      "\n",
      "        kw_papers[kw1] = {\n",
      "            \"papers\": len(word_files[kw1])\n",
      "        }\n",
      "        kw_matrix[kw1] = {}\n",
      "\n",
      "        for kw2 in all_kws:\n",
      "            if kw2 != kw1:\n",
      "                papers_with_both = len(files_with_both_keywords(kw1,kw2))\n",
      "                if papers_with_both > 1:\n",
      "                    kw_matrix[kw1][kw2] = papers_with_both\n",
      "\n",
      "        #if len(kw_matrix[kw1]) == 0:\n",
      "            #del kw_matrix[kw1]\n",
      "            #del kw_papers[kw1]\n",
      "            \n",
      "    return kw_matrix, kw_papers\n",
      "\n",
      "all_kws = labels\n",
      "\n",
      "kw_matrix, kw_papers = calculate_kw_matrix()\n",
      "\n",
      "clear_output()\n",
      "print \"Done. There are %d keywords in matrix.\" % len(kw_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json.dump(kw_matrix, open(\"../FDG/keyword_matrix_kmeans.json\",\"w\"), sort_keys=True, indent=2, separators=(',', ': '))\n",
      "json.dump(kw_papers, open(\"../FDG/keywords_kmeans.json\", \"w\"), sort_keys=True, indent=2, separators=(',', ': '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}