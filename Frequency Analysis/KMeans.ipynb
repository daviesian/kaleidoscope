{
 "metadata": {
  "name": "",
  "signature": "sha256:132899541eccd1d8f7d39ab6ce4717b0d7d3b8355fb844e634b5585f7d028beb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import re\n",
      "from collections import Counter\n",
      "import os\n",
      "import json\n",
      "from IPython.display import clear_output\n",
      "import sys\n",
      "from math import sqrt, log\n",
      "\n",
      "from stemming.porter2 import stem\n",
      "\n",
      "from pprint import pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_dir = \"E:/LocalData/PubMed/Cell/\"\n",
      "\n",
      "stop_words = [w.replace(\"\\n\", \"\") for w in open(\"stop_words.txt\")]\n",
      "\n",
      "unstemmed = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def file_root(f):\n",
      "    tree = ET.parse(f)\n",
      "    return tree.getroot()\n",
      "\n",
      "def get_abstract_node(root):\n",
      "    return root.iter(\"abstract\").next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(node):\n",
      "\n",
      "    ts = [get_text(n) for n in node]\n",
      "    \n",
      "    txts = [node_text for sublist in ts for node_text in sublist]\n",
      "\n",
      "    if node.text:\n",
      "        txts.append(node.text)\n",
      "    return txts\n",
      "\n",
      "def is_float(f):\n",
      "    try:\n",
      "        float(f)\n",
      "        return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "def words(text_list):\n",
      "    word_list = [re.split('\\W+',t) for t in text_list]\n",
      "    \n",
      "    words = [unicode(word).lower() for text in word_list for word in text]\n",
      "    \n",
      "    # Filter out words that are too short.\n",
      "    words = [w for w in words if len(w) > 2]\n",
      "    \n",
      "    # Filter out words that are just numbers.\n",
      "    words = [w for w in words if not is_float(w)]\n",
      "    \n",
      "    # Filter out words that start with a number.\n",
      "    words = [w for w in words if not is_float(w[0])]\n",
      "    \n",
      "    # Filter out stop words\n",
      "    words = [w for w in words if not w in stop_words]\n",
      "    \n",
      "    # Do stemming\n",
      "    stemmed = [(w, stem(w)) for w in words]\n",
      "    \n",
      "    # Save map of stem -> original\n",
      "    for w,s in stemmed:\n",
      "        unstemmed[s] = w\n",
      "    \n",
      "    return [s for w,s in stemmed]\n",
      "\n",
      "def normalise_counter(c):\n",
      "    total_words = sum(c.values())\n",
      "    \n",
      "    for a in c:\n",
      "        c[a] /= float(total_words)\n",
      "\n",
      "def process_file(f):\n",
      "    \n",
      "    root = file_root(f)\n",
      "    \n",
      "    try:\n",
      "        abstract = get_abstract_node(root)\n",
      "    except:\n",
      "        return Counter()\n",
      "    \n",
      "    all_words = words(get_text(abstract))\n",
      "    \n",
      "    return Counter(all_words)\n",
      "\n",
      "def process_all_files():\n",
      "    \n",
      "    corpus_words = Counter()\n",
      "    \n",
      "    files = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
      "\n",
      "    word_files = {}\n",
      "    file_words = {}\n",
      "    for f in files:\n",
      "        clear_output()\n",
      "        print \"Processing %s\" % f\n",
      "        sys.stdout.flush()\n",
      "        \n",
      "        file_words[f] = process_file(f)\n",
      "        corpus_words.update(file_words[f])\n",
      "        for k in file_words[f]:\n",
      "            word_files[k] = word_files.get(k, set())\n",
      "            word_files[k].add(f)\n",
      "            \n",
      "    return file_words, corpus_words, word_files, files\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_words, corpus_words, word_files, files = process_all_files()\n",
      "\n",
      "print \"Computed word count for entire corpus\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing E:/LocalData/PubMed/Cell/Cell_2014_May_8_157(4)_823-831.nxml\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Computed word count for entire corpus\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Per-paper feature vector\n",
      "\n",
      "For the feature vector, only keep words that occur in at least 3 papers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Only allow words that occur in at least three papers\n",
      "\n",
      "allowed_words = [w for w in word_files if len(word_files[w]) >= 3]\n",
      "\n",
      "feature_vectors = []\n",
      "for f in files:\n",
      "    counter = Counter()\n",
      "    for w in file_words[f]:\n",
      "        if w in allowed_words:\n",
      "            counter[w] = file_words[f][w]\n",
      "            \n",
      "    feature_vectors.append(counter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# K-Means"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance(fv1, fv2):\n",
      "    all_features = set([f for f in fv1]).union([f for f in fv2])\n",
      "    \n",
      "    s = 0\n",
      "    for f in all_features:\n",
      "        s += (fv1.get(f,0) - fv2.get(f,0)) ** 2\n",
      "        \n",
      "    return sqrt(s)\n",
      "\n",
      "def centroid(fvs):\n",
      "    \n",
      "    centre = Counter()\n",
      "    \n",
      "    for f in fvs:\n",
      "        centre += f\n",
      "        \n",
      "    for a in centre:\n",
      "        centre[a] /= float(len(fvs))\n",
      "        \n",
      "    return centre\n",
      "\n",
      "def nearest_centroid_index(fv, centroids):\n",
      "    \n",
      "    min_dist = distance(fv, centroids[0])\n",
      "    nearest_index = 0\n",
      "    \n",
      "    for i in range(1, len(centroids)):\n",
      "        d = distance(fv, centroids[i])\n",
      "        if d < min_dist:\n",
      "            min_dist = d\n",
      "            nearest_index = i\n",
      "    \n",
      "    return nearest_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def kmeans(xs,K):\n",
      "    \"\"\"xs is a vector of Counters. K is the required number of clusters\"\"\"\n",
      "    \n",
      "    N = len(xs)\n",
      "    seeds = xs[:K] # Pick first K papers as seed cluster centroids\n",
      "\n",
      "    # Initialise centroids\n",
      "    mu = seeds\n",
      "\n",
      "    # Initialise cluster members\n",
      "    omega = []\n",
      "    for i in range(K):\n",
      "        omega.append(set())\n",
      "\n",
      "\n",
      "    last_omega = omega[:]\n",
      "    i = 0\n",
      "    while i < 20:\n",
      "        for k in range(K):\n",
      "            omega[k] = set()\n",
      "\n",
      "        # Reassign xs to nearest centroid\n",
      "\n",
      "        for n in range(N):\n",
      "            j = nearest_centroid_index(xs[n], mu)\n",
      "            omega[j].add(n)\n",
      "\n",
      "        # Recompute cluster centres\n",
      "\n",
      "        for k in range(K):\n",
      "            mu[k] = centroid([xs[t] for t in omega[k]])\n",
      "\n",
      "\n",
      "        match = True\n",
      "        for t in range(len(omega)):\n",
      "            if omega[t] != last_omega[t]:\n",
      "                match = False\n",
      "                break\n",
      "\n",
      "        if match:\n",
      "            print \"\\nConverged in %d iterations.\\n\\n\" % i\n",
      "            break \n",
      "\n",
      "        print\n",
      "        print str(i) + \": \" + str(omega)\n",
      "\n",
      "        i += 1\n",
      "\n",
      "        last_omega = omega[:]\n",
      "    \n",
      "    return omega\n",
      "    \n",
      "\n",
      "def mutual_information(word, cluster, files, feature_vectors):\n",
      "    N = float(len(files))\n",
      "    \n",
      "    # The number of files not containing the word and not in the cluster\n",
      "    N00 = float(len([f for fi,f in enumerate(files) if not fi in cluster and not word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files containing the word and in the cluster\n",
      "    N11 = float(len([f for fi,f in enumerate(files) if fi in cluster and word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files containing the word, but not in the cluster\n",
      "    N10 = float(len([f for fi,f in enumerate(files) if not fi in cluster and word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files not containing the word, but in the cluster\n",
      "    N01 = float(len([f for fi,f in enumerate(files) if fi in cluster and not word in feature_vectors[fi]]))\n",
      "    \n",
      "    # The number of files containing the word\n",
      "    N1_ = N11 + N10\n",
      "    \n",
      "    # The number of files not containing the word\n",
      "    N0_ = N00 + N01\n",
      "    \n",
      "    # The number of files in the cluster\n",
      "    N_1 = N01 + N11\n",
      "    \n",
      "    # The number of files not in the cluster\n",
      "    N_0 = N00 + N10\n",
      "    \n",
      "    #print N00, N01, N10, N11\n",
      "    \n",
      "    mi = 0\n",
      "    if N11 > 0:\n",
      "        mi += N11/N * log(N*N11/(N1_*N_1),2)\n",
      "    if N01 > 0:\n",
      "        mi += N01/N * log(N*N01/(N0_*N_1),2)\n",
      "    if N10 > 0:\n",
      "        mi += N10/N * log(N*N10/(N1_*N_0),2)\n",
      "    if N00 > 0:\n",
      "        mi += N00/N * log(N*N00/(N0_*N_0),2)\n",
      "    \n",
      "    return mi\n",
      "         \n",
      "    \n",
      "def cluster_label(cluster, feature_vectors, files):\n",
      "    \n",
      "    cluster_words = Counter()\n",
      "    for fi in cluster:\n",
      "        cluster_words += feature_vectors[fi]\n",
      "        \n",
      "    for w in cluster_words:\n",
      "        cluster_words[w] = mutual_information(w, cluster, files, feature_vectors)\n",
      "    \n",
      "    return cluster_words.most_common(1)[0][0]\n",
      "            \n",
      "          \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "clusters = kmeans(feature_vectors, 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0: [set([0]), set([1]), set([2]), set([3]), set([4, 71, 105, 103, 12, 77, 111, 92, 62]), set([5, 63]), set([6]), set([104, 97, 101, 78, 7]), set([8]), set([9]), set([10]), set([11]), set([]), set([13]), set([14]), set([15]), set([16]), set([17, 89]), set([18]), set([19]), set([20]), set([21]), set([64, 22]), set([23]), set([24]), set([25, 86, 95]), set([26]), set([27]), set([67, 68, 108, 107, 76, 79, 28]), set([29]), set([30]), set([31]), set([32, 60]), set([33]), set([34]), set([35, 48, 51, 52, 53, 55, 56, 57, 58, 59, 65, 66, 69, 70, 74, 80, 81, 82, 84, 85, 87, 90, 91, 93, 94, 98, 100, 102, 110, 112]), set([72, 96, 36, 54]), set([37]), set([38]), set([39]), set([40]), set([41]), set([42]), set([75, 43]), set([44]), set([106, 45]), set([109, 46]), set([99, 73, 47, 50, 83, 88, 61]), set([]), set([49])]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1: [set([0]), set([1]), set([2]), set([3]), set([105, 103, 92, 62, 71]), set([5, 63]), set([6]), set([104, 97, 101, 78, 7]), set([8]), set([9]), set([10]), set([11]), set([4, 12, 17, 22, 25, 28, 35, 36, 43, 47, 48, 52, 59, 65, 66, 74, 77, 81, 82, 88, 90, 91, 94, 100, 111]), set([13]), set([14]), set([15]), set([16]), set([89]), set([18]), set([19]), set([20]), set([21]), set([64]), set([23]), set([24]), set([84, 86, 95]), set([26]), set([27]), set([67, 68, 108, 107, 76, 79]), set([29]), set([30]), set([31]), set([32, 60, 61]), set([33]), set([34]), set([112, 98, 69, 70, 87, 110, 80, 102, 51, 53, 55, 56, 57, 58, 93, 85]), set([72, 96, 54]), set([37]), set([38]), set([39]), set([40]), set([41]), set([42]), set([75]), set([44]), set([106, 45]), set([109, 46]), set([99, 73, 50, 83]), set([]), set([49])]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2: [set([0]), set([1]), set([2]), set([3]), set([103, 62, 71]), set([5, 63]), set([6]), set([104, 97, 101, 78, 7]), set([8]), set([9]), set([10]), set([11]), set([65, 99, 36, 92, 74, 43, 108, 111, 80, 17, 82, 85, 22, 105, 88, 57, 91, 28]), set([13]), set([14]), set([15]), set([16]), set([89]), set([18]), set([19]), set([20]), set([21]), set([64]), set([23]), set([24]), set([84, 86, 95]), set([26]), set([27]), set([107, 76, 67, 68, 79]), set([29]), set([30]), set([31]), set([32, 60, 61, 93]), set([33]), set([34]), set([98, 69, 70, 87, 110, 112, 51, 53, 55, 56, 58]), set([72, 96, 54]), set([37]), set([38]), set([39]), set([40]), set([41]), set([42]), set([75]), set([44]), set([106, 45]), set([109, 46]), set([73, 50, 83]), set([100, 66, 35, 4, 102, 12, 77, 47, 48, 81, 52, 25, 90, 59, 94]), set([49])]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3: [set([0]), set([1]), set([2]), set([3]), set([71]), set([5, 63]), set([6]), set([104, 97, 101, 78, 7]), set([8]), set([9]), set([10]), set([11]), set([65, 99, 36, 103, 92, 74, 43, 108, 80, 17, 51, 85, 22, 105, 88, 57, 91, 28, 62]), set([13]), set([14]), set([15]), set([16]), set([89]), set([18]), set([19]), set([20]), set([21]), set([64]), set([23]), set([24]), set([84, 86, 95]), set([26]), set([27]), set([107, 76, 67, 68, 79]), set([29]), set([30]), set([31]), set([32, 60, 61, 93]), set([33]), set([34]), set([98, 69, 70, 87, 110, 53, 55, 56, 58]), set([72, 96, 54]), set([37]), set([38]), set([39]), set([40]), set([41]), set([42]), set([75]), set([44]), set([106, 45]), set([109, 46]), set([73, 50, 83]), set([100, 66, 35, 4, 102, 112, 111, 12, 77, 47, 48, 81, 82, 52, 25, 90, 59, 94]), set([49])]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4: [set([0]), set([1]), set([2]), set([3]), set([71]), set([5, 63]), set([6]), set([104, 97, 101, 78, 7]), set([8]), set([9]), set([10]), set([11]), set([65, 99, 36, 103, 92, 74, 43, 108, 80, 17, 51, 105, 85, 22, 87, 88, 57, 91, 28, 62]), set([13]), set([14]), set([15]), set([16]), set([89]), set([18]), set([19]), set([20]), set([21]), set([64]), set([23]), set([24]), set([84, 86, 95]), set([26]), set([27]), set([107, 76, 67, 68, 79]), set([29]), set([30]), set([31]), set([32, 60, 61, 93]), set([33]), set([34]), set([98, 69, 70, 110, 53, 55, 58]), set([72, 96, 54]), set([37]), set([38]), set([39]), set([40]), set([41]), set([42]), set([75]), set([44]), set([106, 45]), set([109, 46]), set([73, 50, 83]), set([100, 66, 35, 4, 102, 112, 111, 12, 77, 47, 48, 81, 82, 52, 56, 25, 90, 59, 94]), set([49])]\n",
        "\n",
        "Converged in 5 iterations.\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels = []\n",
      "for cluster in clusters:\n",
      "    if len(cluster) < 2:\n",
      "        continue # Ignore singleton clusters\n",
      "        \n",
      "    label = cluster_label(cluster, feature_vectors, files)\n",
      "    labels.append(label)\n",
      "    \n",
      "pprint(labels)\n",
      "\n",
      "#mutual_information(\"protein\", clusters[7], files, file_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'fork',\n",
        " u'capac',\n",
        " u'pirna',\n",
        " u'substrat',\n",
        " u'protein',\n",
        " u'somat',\n",
        " u'base',\n",
        " u'membran',\n",
        " u'polycomb',\n",
        " u'compart',\n",
        " u'repres',\n",
        " u'mechan']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This used to generate labels based on most common word in cluster.\n",
      "\n",
      "#labels =  []\n",
      "#\n",
      "#for k in range(len(clusters)):\n",
      "#    total_word_freq = Counter()\n",
      "#    \n",
      "#    if len(clusters[k]) < 2:\n",
      "#        continue # Ignore clusters with only one paper\n",
      "#        \n",
      "#    for fi in clusters[k]:\n",
      "#        total_word_freq += file_words[files[fi]]\n",
      "#    \n",
      "#    labels.append(total_word_freq.most_common(1)[0][0])\n",
      "#\n",
      "#pprint(labels)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def files_with_both_keywords(kw1, kw2):\n",
      "    return word_files[kw1].intersection(word_files[kw2])\n",
      "\n",
      "\n",
      "def calculate_kw_matrix():\n",
      "\n",
      "    kw_matrix = {}\n",
      "    kw_papers = {}\n",
      "\n",
      "    for kw1 in all_kws:\n",
      "        clear_output()\n",
      "        print \"Testing %s\" % kw1\n",
      "        sys.stdout.flush()\n",
      "\n",
      "        kw_papers[kw1] = {\n",
      "            \"papers\": len(word_files[kw1])\n",
      "        }\n",
      "        kw_matrix[kw1] = {}\n",
      "\n",
      "        for kw2 in all_kws:\n",
      "            if kw2 != kw1:\n",
      "                papers_with_both = len(files_with_both_keywords(kw1,kw2))\n",
      "                if papers_with_both > 1:\n",
      "                    kw_matrix[kw1][kw2] = papers_with_both\n",
      "\n",
      "        #if len(kw_matrix[kw1]) == 0:\n",
      "            #del kw_matrix[kw1]\n",
      "            #del kw_papers[kw1]\n",
      "            \n",
      "    return kw_matrix, kw_papers\n",
      "\n",
      "all_kws = labels\n",
      "\n",
      "kw_matrix, kw_papers = calculate_kw_matrix()\n",
      "\n",
      "clear_output()\n",
      "print \"Done. There are %d keywords in matrix.\" % len(kw_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done. There are 12 keywords in matrix.\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json.dump(kw_matrix, open(\"../FDG/keyword_matrix_kmeans.json\",\"w\"), sort_keys=True, indent=2, separators=(',', ': '))\n",
      "json.dump(kw_papers, open(\"../FDG/keywords_kmeans.json\", \"w\"), sort_keys=True, indent=2, separators=(',', ': '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "-13.815510557964274"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}