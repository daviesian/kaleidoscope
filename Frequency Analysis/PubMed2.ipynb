{
 "metadata": {
  "name": "",
  "signature": "sha256:7a6788589b0584c3e8846564b6e64b828a2f1aca93d9b5f50863aa07cee6f765"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import re\n",
      "from collections import Counter\n",
      "import os\n",
      "import json\n",
      "from IPython.display import clear_output\n",
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_dir = \"E:/LocalData/PubMed/Cell/\"\n",
      "\n",
      "stop_words = [w.replace(\"\\n\", \"\") for w in open(\"stop_words.txt\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def file_root(f):\n",
      "    tree = ET.parse(f)\n",
      "    return tree.getroot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(node):\n",
      "\n",
      "    ts = [get_text(n) for n in node]\n",
      "    \n",
      "    txts = [node_text for sublist in ts for node_text in sublist]\n",
      "\n",
      "    if node.text:\n",
      "        txts.append(node.text)\n",
      "    return txts\n",
      "\n",
      "def is_float(f):\n",
      "    try:\n",
      "        float(f)\n",
      "        return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "def words(text_list):\n",
      "    word_list = [re.split('\\W+',t) for t in text_list]\n",
      "    \n",
      "    words = [unicode(word).lower() for text in word_list for word in text]\n",
      "    \n",
      "    # Filter out words that are too short.\n",
      "    words = [w for w in words if len(w) > 2]\n",
      "    \n",
      "    # Filter out words that are just numbers.\n",
      "    words = [w for w in words if not is_float(w)]\n",
      "    \n",
      "    # Filter out words that start with a number.\n",
      "    words = [w for w in words if not is_float(w[0])]\n",
      "    \n",
      "    # Filter out stop words\n",
      "    words = [w for w in words if not w in stop_words]\n",
      "    \n",
      "    return words\n",
      "\n",
      "def normalise_counter(c):\n",
      "    total_words = sum(c.values())\n",
      "    \n",
      "    for a in c:\n",
      "        c[a] /= float(total_words)\n",
      "\n",
      "def process_file(f):\n",
      "    \n",
      "    root = file_root(f)\n",
      "    \n",
      "    all_words = words(get_text(root))\n",
      "    \n",
      "    return Counter(set(all_words))\n",
      "\n",
      "def process_all_files():\n",
      "    \n",
      "    total_tally = Counter()\n",
      "    \n",
      "    files = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
      "\n",
      "    kw_files = {}\n",
      "    file_kws = {}\n",
      "    for f in files:\n",
      "        clear_output()\n",
      "        print \"Processing %s\" % f\n",
      "        sys.stdout.flush()\n",
      "        \n",
      "        file_words = process_file(f)\n",
      "        total_tally.update(file_words)\n",
      "        file_kws[f] = [k for k in file_words]\n",
      "        for k in file_words:\n",
      "            kw_files[k] = kw_files.get(k, set())\n",
      "            kw_files[k].add(f)\n",
      "            \n",
      "    return file_kws, total_tally, kw_files\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = set([1,2,3])\n",
      "a.add(4)\n",
      "a.remove(2)\n",
      "a & set([1,2])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fs, total_tally, kw_files = process_all_files()\n",
      "\n",
      "print \"Computed word count for entire corpus\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total_tally"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For every possible keyword, what percentage of papers have that keyword.\n",
      "\n",
      "# Hypothesis: If it's between 20% and 50%, this might be a useful keyword for discriminating between topics.\n",
      "# Result: No. It just doesn't work.\n",
      "\n",
      "all_kws = []\n",
      "for k in total_tally:\n",
      "    paper_count = total_tally[k]\n",
      "    perc = 100 * paper_count / float(len(fs))\n",
      "    #print k + \" : \" + str(perc) + \"%\"\n",
      "    if perc > 20 and perc < 50:\n",
      "        all_kws.append(k)\n",
      "\n",
      "all_kws.sort()\n",
      "print \"Found %d keywords.\" % len(all_kws)\n",
      "#print all_kws"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def files_with_both_keywords(kw1, kw2):\n",
      "    return kw_files[kw1].intersection(kw_files[kw2])\n",
      "\n",
      "\n",
      "def calculate_kw_matrix():\n",
      "\n",
      "    kw_matrix = {}\n",
      "    kw_papers = {}\n",
      "\n",
      "    for kw1 in all_kws:\n",
      "        clear_output()\n",
      "        print \"Testing %s\" % kw1\n",
      "        sys.stdout.flush()\n",
      "\n",
      "        kw_papers[kw1] = {\n",
      "            \"papers\": len(kw_files[kw1])\n",
      "        }\n",
      "        kw_matrix[kw1] = {}\n",
      "\n",
      "        for kw2 in all_kws:\n",
      "            if kw2 != kw1:\n",
      "                papers_with_both = len(files_with_both_keywords(kw1,kw2))\n",
      "                if papers_with_both > 30:\n",
      "                    kw_matrix[kw1][kw2] = papers_with_both\n",
      "\n",
      "        if len(kw_matrix[kw1]) == 0:\n",
      "            del kw_matrix[kw1]\n",
      "            del kw_papers[kw1]\n",
      "            \n",
      "    return kw_matrix, kw_papers\n",
      "\n",
      "kw_matrix, kw_papers = calculate_kw_matrix()\n",
      "\n",
      "clear_output()\n",
      "print \"Done. There are %d keywords in matrix.\" % len(kw_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json.dump(kw_matrix, open(\"../FDG/keyword_matrix_2.json\",\"w\"), sort_keys=True, indent=2, separators=(',', ': '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json.dump(kw_papers, open(\"../FDG/keywords_2.json\", \"w\"), sort_keys=True, indent=2, separators=(',', ': '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}