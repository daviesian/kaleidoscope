{
 "metadata": {
  "name": "",
  "signature": "sha256:ea3687513eb8380114fbc151b6fa9827faf2d98ca6fd96b52e1f78b9de0cf6ec"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import re\n",
      "from collections import Counter\n",
      "import os\n",
      "import json\n",
      "from IPython.display import clear_output\n",
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_dir = \"E:/LocalData/PubMed/Cell/\"\n",
      "\n",
      "stop_words = [w.replace(\"\\n\", \"\") for w in open(\"stop_words.txt\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def file_root(f):\n",
      "    tree = ET.parse(f)\n",
      "    return tree.getroot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text(node):\n",
      "\n",
      "    ts = [get_text(n) for n in node]\n",
      "    \n",
      "    txts = [node_text for sublist in ts for node_text in sublist]\n",
      "\n",
      "    if node.text:\n",
      "        txts.append(node.text)\n",
      "    return txts\n",
      "\n",
      "def is_float(f):\n",
      "    try:\n",
      "        float(f)\n",
      "        return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "def words(text_list):\n",
      "    word_list = [re.split('\\W+',t) for t in text_list]\n",
      "    \n",
      "    words = [unicode(word).lower() for text in word_list for word in text]\n",
      "    \n",
      "    # Filter out words that are too short.\n",
      "    words = [w for w in words if len(w) > 2]\n",
      "    \n",
      "    # Filter out words that are just numbers.\n",
      "    words = [w for w in words if not is_float(w)]\n",
      "    \n",
      "    # Filter out words that start with a number.\n",
      "    words = [w for w in words if not is_float(w[0])]\n",
      "    \n",
      "    # Filter out stop words\n",
      "    words = [w for w in words if not w in stop_words]\n",
      "    \n",
      "    return words\n",
      "\n",
      "def normalise_counter(c):\n",
      "    total_words = sum(c.values())\n",
      "    \n",
      "    for a in c:\n",
      "        c[a] /= float(total_words)\n",
      "\n",
      "def process_file(f):\n",
      "    \n",
      "    root = file_root(f)\n",
      "    \n",
      "    all_words = words(get_text(root))\n",
      "    \n",
      "    return Counter(all_words)\n",
      "\n",
      "def process_all_files():\n",
      "    \n",
      "    total_tally = Counter()\n",
      "    \n",
      "    files = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
      "\n",
      "\n",
      "    file_counts = {}\n",
      "    for f in files:\n",
      "        file_counts[f] = process_file(f)\n",
      "        total_tally.update(file_counts[f])\n",
      "        \n",
      "        normalise_counter(file_counts[f])\n",
      "        \n",
      "    normalise_counter(total_tally)\n",
      "    \n",
      "    return file_counts, total_tally\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fs, total_tally = process_all_files()\n",
      "#total_tally.most_common()\n",
      "\n",
      "print \"Computed word frequencies for entire corpus\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Computed word frequencies for entire corpus\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use dodgy percentage comparison to decide on 50 most important keywords in each paper.\n",
      "# (Important means significantly more common than in the corpus)\n",
      "\n",
      "def compute_diffs():\n",
      "    file_kws = {}\n",
      "    for f in fs:\n",
      "        diffs = Counter()\n",
      "        for w in fs[f]:\n",
      "            diffs[w] = fs[f][w] - total_tally[w]\n",
      "\n",
      "        file_kws[f] = [w for w,_ in diffs.most_common()[:50]]\n",
      "    \n",
      "    return file_kws\n",
      "    \n",
      "file_kws = compute_diffs()\n",
      "\n",
      "all_kws = list(set([kw for sublist in [lst for f,lst in file_kws.iteritems()] for kw in sublist]))\n",
      "all_kws.sort()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def files_with_both_keywords(kw1, kw2):\n",
      "    return [f for f,lst in file_kws.iteritems() if kw1 in lst and kw2 in lst]\n",
      "\n",
      "# Keyword matrix tells us, for every pair of keywords, how many papers contain both.\n",
      "\n",
      "def calculate_kw_matrix():\n",
      "\n",
      "    kw_matrix = {}\n",
      "    kw_papers = {}\n",
      "\n",
      "    for kw1 in all_kws:\n",
      "        clear_output()\n",
      "        print \"Testing %s\" % kw1\n",
      "        sys.stdout.flush()\n",
      "\n",
      "        papers_with_keyword = len([f for f,lst in file_kws.iteritems() if kw1 in lst])\n",
      "        \n",
      "        if (papers_with_keyword > 1):\n",
      "            kw_papers[kw1] = {\n",
      "                \"papers\": papers_with_keyword\n",
      "            }\n",
      "            kw_matrix[kw1] = {}\n",
      "\n",
      "            for kw2 in all_kws:\n",
      "                if kw2 != kw1:\n",
      "                    papers_with_both = len(files_with_both_keywords(kw1,kw2))\n",
      "                    if papers_with_both > 1:\n",
      "                        kw_matrix[kw1][kw2] = papers_with_both\n",
      "        \n",
      "        \n",
      "            if len(kw_matrix[kw1]) == 0:\n",
      "                del kw_matrix[kw1]\n",
      "                del kw_papers[kw1]\n",
      "    return kw_matrix, kw_papers\n",
      "\n",
      "kw_matrix, kw_papers = calculate_kw_matrix()\n",
      "\n",
      "clear_output()\n",
      "print \"Done. There are %d keywords in matrix.\" % len(kw_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done. There are 861 keywords in matrix.\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json.dump(kw_matrix, open(\"../FDG/keyword_matrix.json\",\"w\"), sort_keys=True, indent=2, separators=(',', ': '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json.dump(kw_papers, open(\"../FDG/keywords.json\", \"w\"), sort_keys=True, indent=2, separators=(',', ': '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}